Ollama Chat
---

Integrate a locally running Ollama as a copilot. This extension provides a 
user-friendly chat interface, allowing you to interact with Ollama directly 
within VSCode for coding assistance.

## Features
- **Local Ollama Integration**: Utilizes your locally installed Ollama instance for real-time code suggestions and assistance.
- **Chat Interface**: A dedicated chat pane within VSCode where you can communicate with Ollama and receive responses seamlessly.

## Getting Started
1. **Install the Extension**:
   - Open VSCode.
   - Navigate to the Extensions view (`Ctrl+Shift+X`).
   - Search for "Ollama Chat".
   - Click on "Install".

2. **Setup Local Ollama**:
   - Ensure that you have Ollama installed and running locally on your machine ([link](https://ollama.com)).
   - Make sure your Ollama server runs at `http://127.0.0.1:11434`

3. **Using the Extension**:
   - Open the Command Palette and look for `Focus on Ollama Chat view`

## Support and Feedback
For any issues or suggestions, please [open an issue](https://github.com/shikaan/ollama-chat/issues).
